## Title of the Project
SIGN LANGUAGE RECOGNITION WITH COMPUTER VISION AND MEDIA PIPE

## About
Visually impaired individuals have limited access to information compared to those with normal vision. A significant lack of inclusivity has created a wide information gap, as the modern world is not designed with visually impaired people in mind. Much essential technological Information is typically not available in Braille, as most documents are in human languages. This project aims to develop a translation system where the conversion of human language to its subsequent braille is done. By conversion of language into braille seamlessly, information is also available to the visually impaired people in their language. This project supports various formats like photo, documents and pdf in human language which needs to be translated. Mapping of human language to its braille takes place through both normal mapping processes and also through deep learning. Deep learning plays a pivotal role in the conversion of the English language to its braille by training it through a vast amount of dataset. Both deep learning and mapping techniques are used in conversion of languages to their braille form. The primary goal of this project is to provide a working functional system, ensuring that visually impaired individuals have unrestricted access to information in their braille language and act as a translation system.
## Features
1)Accessibility Enhancement. 
2)Communication Aid for Deaf Individuals.
3)Real-time Translation Services. 
4)Integration with Assistive Technologies.
5)Open-source Contribution.

## Requirements
Hardware Requirements

Graphics Processing Unit (GPU):
Dedicated GPU: A dedicated graphics processing unit, specifically an NVIDIA GeForce RTX Series.
Memory (RAM):
RAM Capacity: A minimum of 8GB of RAM is essential to meet the computational demands of the system, particularly for tasks related to neural network inference, image processing, and real-time analysis.
High-Resolution Display:
Camera Quality: An HD Webcam is an integral part of the system.
Software Requirements
The software requirements for the successful deployment of the text recognition system are as follows:
*Python:
Python is a versatile and widely used programming language. Many computer vision and machine learning libraries are available for Python, making it a suitable choice for this project.
*Jupyter Notebooks:
Jupyter Notebooks provide an interactive computing environment that allows you to create and share documents containing live code, equations, visualizations, and narrative text. They are excellent for data exploration, experimentation, and collaboration.
*Media Pipe:
Google's Media Pipe is a powerful framework for building perception pipelines. It includes pre-trained models for hand tracking, which is crucial for sign language recognition.
*OpenCV (Open Source Computer Vision): OpenCV is an open-source computer vision library. It provides a wide range of tools and functions for image and video processing, making it valuable for tasks such as capturing video input and processing frames.

## System Architecture
![image](https://github.com/user-attachments/assets/62c92b02-54a8-4d4d-9ab9-67ac5d3b01fa)

## Output

#### Output1 - Thumbs up

![image](https://github.com/user-attachments/assets/45fd01c6-5a93-4daf-842f-c03e1841754a)

Detection Accuracy: 97%


## Results and Impact
The Sign Language Detection System enhances accessibility for individuals with hearing and speech impairments, providing a valuable tool for inclusive communication. The project's integration of computer vision and deep learning showcases its potential for intuitive and interactive human-computer interaction.In conclusion, the Sign Language Interpreter project represents a meaningful step towards creating inclusive technology. The collaboration of computer vision and machine learning in this context has opened new avenues for accessibility. We extend our gratitude to the developers of the Media Pipe library, as well as the broader open-source community, for providing the tools and resources that made this project possible. The journey doesn't end here; rather, it marks the beginning of a commitment to advancing technology for the betterment of society.

This project serves as a foundation for future developments in assistive technologies and contributes to creating a more inclusive and accessible digital environment.

## Articles published / References
[1] Simei G. Wysoski, Marcus V. Lamar, Susumu Kuroyanagi, Akira Iwata, (2002). “A Rotation Invariant Approach On Static-Gesture Recognition Using Boundary Histograms And Neural International Journal of Artificial Intelligence & Applications (IJAIA), Vol.3, No.4, July 2012 173 Networks,” IEEE Proceedings of the 9th International Conference on Neural Information Processing, Singapura.. 
[2] G. R. S. Murthy, R. S. Jadon. (2009). “A Review of Vision Based Hand Gestures Recognition,” International Journal of Information Technology and Knowledge Management, vol. 2(2), pp. 405- 410. 
[3] P. Garg, N. Aggarwal and S. Sofat. (2009). “Vision Based Hand Gesture Recognition,” World Academy of Science, Engineering and Technology, Vol. 49, pp. 972-977.


